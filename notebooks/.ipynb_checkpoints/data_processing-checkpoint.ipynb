{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32919920-2da4-449c-aac8-1c5a964fa1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des bibliotheques\n",
    "import pandas as pd #pour lire et manipuler les fichiers csv\n",
    "import numpy as np #pour le calcul numeriques\n",
    "import os #pout le chemin aux datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275ba154-4a92-47fa-8f31-7e3c54159e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fichiers disponible dans le dossier datasets : ['.ipynb_checkpoints', 'account.csv', 'bill.csv', 'changes.csv', 'history.csv', 'proceed.csv', 'splogin_history.csv', 'transfert.csv', 'user.csv']\n"
     ]
    }
   ],
   "source": [
    "#definition du chemin du dossier contenant les datasets\n",
    "data_path = \"datasets\" \n",
    "\n",
    "#lister tous des fichiers presents dans le dossier datasets\n",
    "print(\"fichiers disponible dans le dossier datasets :\", os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271fbe22-0b8f-49b1-8533-b097085db323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tous les fichiers CSV ont été chargés avec succès !\n"
     ]
    }
   ],
   "source": [
    "#chargement des fichiers csv \n",
    "account_df = pd.read_csv(os.path.join(data_path, \"account.csv\"))\n",
    "changes_df = pd.read_csv(os.path.join(data_path, \"changes.csv\"))\n",
    "splogin_history_df = pd.read_csv(os.path.join(data_path, \"splogin_history.csv\"))\n",
    "transfert_df = pd.read_csv(os.path.join(data_path, \"transfert.csv\"))\n",
    "user_df = pd.read_csv(os.path.join(data_path, \"user.csv\"))\n",
    "proceed_df = pd.read_csv(os.path.join(data_path, \"proceed.csv\"))\n",
    "history_df = pd.read_csv(os.path.join(data_path, \"history.csv\"), dtype=str)\n",
    "bill_df = pd.read_csv(os.path.join(data_path, \"bill.csv\"))\n",
    "\n",
    "print(\"\\n Tous les fichiers CSV ont été chargés avec succès !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfc8c93-47a7-4732-a57d-94ec1f402341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Aperçu du fichier account.csv :\n",
      "   id  user_id  currency_id     wallet    balance           updated_at  \\\n",
      "0  36        8          4.0        NaN      0.000  2021-07-13 16:31:35   \n",
      "1  37        8          3.0        NaN      0.000  2021-07-13 16:39:12   \n",
      "2  38        8          2.0        NaN      0.000  2021-07-13 16:41:29   \n",
      "3  39        8          1.0  671605916      0.000  2021-07-13 16:43:53   \n",
      "4  40       14          1.0  SP202114A  12254.553  2025-02-23 01:51:37   \n",
      "\n",
      "   trade_balance  pending_balance  \n",
      "0            NaN              NaN  \n",
      "1            NaN              NaN  \n",
      "2            NaN              NaN  \n",
      "3            NaN              NaN  \n",
      "4          500.0            300.0  \n",
      "\n",
      " Aperçu du fichier bill.csv :\n",
      "   id  amount  service_id  operation_id   status  user_id currency  is_verif  \\\n",
      "0   1   250.0           1             2  SUCCESS       14      XAF         1   \n",
      "1   2   250.0           1             2  SUCCESS       14      XAF         1   \n",
      "2   3   100.0           1             2  PENDING       14      XAF         0   \n",
      "3   4  1000.0           1             2  PENDING       14      XAF         0   \n",
      "4   5   500.0           1             2  SUCCESS       14      XAF         1   \n",
      "\n",
      "  ext_id           proceed_at  ... send_back updated_at  waiting_expired_at  \\\n",
      "0  MLS1B  2021-07-14 00:31:32  ...         0        NaN                 NaN   \n",
      "1  MLS2B  2021-07-14 00:51:03  ...         0        NaN                 NaN   \n",
      "2  MLS3B  2021-07-14 18:39:59  ...         0        NaN                 NaN   \n",
      "3  MLS4B  2021-07-18 17:02:08  ...         0        NaN                 NaN   \n",
      "4  MLS5B  2021-07-22 12:43:57  ...         0        NaN                 NaN   \n",
      "\n",
      "  success_url  failure_url customer_id external_reference  payer_email  \\\n",
      "0         NaN          NaN         NaN                NaN          NaN   \n",
      "1         NaN          NaN         NaN                NaN          NaN   \n",
      "2         NaN          NaN         NaN                NaN          NaN   \n",
      "3         NaN          NaN         NaN                NaN          NaN   \n",
      "4         NaN          NaN         NaN                NaN          NaN   \n",
      "\n",
      "   partner_id  fees_paid  \n",
      "0         NaN        NaN  \n",
      "1         NaN        NaN  \n",
      "2         NaN        NaN  \n",
      "3         NaN        NaN  \n",
      "4         NaN        NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      " Aperçu du fichier changes.csv :\n",
      "   id  in_amount  user_id  in_wallet out_wallet  out_amount   status  \\\n",
      "0   1      100.0       15  697935320  677347922        97.0  FAILURE   \n",
      "1   2      100.0       16  677347922  698479723        97.0  SUCCESS   \n",
      "2   3      100.0       15  677347922  697935320        97.0  FAILURE   \n",
      "3   4      100.0       15  677347922  697935320        97.0  FAILURE   \n",
      "4   5      100.0       15  677347922  697935320        97.0  SUCCESS   \n",
      "\n",
      "   in_service_id  out_service_id  is_verif ext_id  operation_id  ext_ref  \\\n",
      "0              2               1         0  MLS1C             3      NaN   \n",
      "1              1               2         1  MLS2C             3      NaN   \n",
      "2              2               1         0  MLS3C             3      NaN   \n",
      "3              2               1         0  MLS4C             3      NaN   \n",
      "4              1               2         1  MLS5C             3      NaN   \n",
      "\n",
      "            proceed_at                                 other  to_account  \\\n",
      "0  2021-12-28 18:57:51  364a3031-2d35-4321-8768-94b66b1ca688         NaN   \n",
      "1  2021-12-28 19:01:23  c35f93d7-3ded-4278-81f0-5edc43f2234d         NaN   \n",
      "2  2021-12-28 19:05:59  44aba632-0166-433a-bd34-d185c8d16a56         NaN   \n",
      "3  2021-12-28 19:08:27  e95da7ee-38d5-47a7-88c0-f7b07e9c36b1         NaN   \n",
      "4  2021-12-28 19:11:55  d431d508-a570-4e77-9ca3-3983c7d0bf0a         NaN   \n",
      "\n",
      "  external_reference  \n",
      "0                NaN  \n",
      "1                NaN  \n",
      "2                NaN  \n",
      "3                NaN  \n",
      "4                NaN  \n",
      "\n",
      " Aperçu du fichier history.csv :\n",
      "  id document                 date    value  status user_id operation_id  \\\n",
      "0  1    MLS1P  2021-01-11 14:25:12  21.5765  FAILED       8            1   \n",
      "1  2    MLS2P  2021-03-30 15:13:34        4  FAILED       8            1   \n",
      "2  3    MLS3P  2021-07-09 00:15:31        1  FAILED       8            1   \n",
      "3  4    MLS4P  2021-07-09 00:28:16        1  FAILED       8            1   \n",
      "4  5    MLS5P  2021-07-09 00:28:38        1  FAILED       8            1   \n",
      "\n",
      "  receiver_matricule account_id ext_id host location city external_reference  \\\n",
      "0                NaN        NaN    NaN  NaN      NaN  NaN                NaN   \n",
      "1                NaN        NaN    NaN  NaN      NaN  NaN                NaN   \n",
      "2                NaN        NaN    NaN  NaN      NaN  NaN                NaN   \n",
      "3                NaN        NaN    NaN  NaN      NaN  NaN                NaN   \n",
      "4                NaN        NaN    NaN  NaN      NaN  NaN                NaN   \n",
      "\n",
      "  invoice_sent prev_account_balance current_account_balance transaction_fees  \\\n",
      "0            1                    0                       0              NaN   \n",
      "1            1                    0                       0              NaN   \n",
      "2            1                    0                       0              NaN   \n",
      "3            1                    0                       0              NaN   \n",
      "4            1                    0                       0              NaN   \n",
      "\n",
      "  completed  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      " Aperçu du fichier proceed.csv :\n",
      "   id  operation_id  service_id  user_id ext_id  amount     wallet currency  \\\n",
      "0   1             1           3        8  MLS1P    10.0       1111      USD   \n",
      "1   2             1           1        8  MLS2P  1000.0  698479723      XAF   \n",
      "2   3             1           1        8  MLS3P   250.0        NaN      XAF   \n",
      "3   4             1           1        8  MLS4P   250.0        NaN      XAF   \n",
      "4   5             1           1        8  MLS5P   250.0        NaN      XAF   \n",
      "\n",
      "    status description  is_verif           proceed_at  \\\n",
      "0  PENDING         NaN         0  2021-01-11 14:25:11   \n",
      "1  PENDING         NaN         0  2021-03-30 15:13:34   \n",
      "2  PENDING         NaN         0  2021-07-09 00:15:30   \n",
      "3  PENDING         NaN         0  2021-07-09 00:28:14   \n",
      "4  PENDING         NaN         0  2021-07-09 00:28:37   \n",
      "\n",
      "                                  other account_name bank_name  otp  \\\n",
      "0                                   NaN          NaN       NaN  NaN   \n",
      "1  36e33926-9cd8-41cb-8429-4748bd32b177          NaN       NaN  NaN   \n",
      "2  e43ad17e-4248-4085-a299-09b59a96954d          NaN       NaN  NaN   \n",
      "3  4b520465-6777-4f52-bc7b-2e329f19ace6          NaN       NaN  NaN   \n",
      "4  cc13e3fc-d6b5-4b43-8e06-0d68bd83f91e          NaN       NaN  NaN   \n",
      "\n",
      "  success_url  failure_url external_reference  \n",
      "0         NaN          NaN                NaN  \n",
      "1         NaN          NaN                NaN  \n",
      "2         NaN          NaN                NaN  \n",
      "3         NaN          NaN                NaN  \n",
      "4         NaN          NaN                NaN  \n",
      "\n",
      " Aperçu du fichier splogin_history.csv :\n",
      "   id  user_id            host                                        country  \\\n",
      "0   1     14.0    142.93.45.42  United Kingdom of Great Britain and Northern    \n",
      "1   2     14.0    142.93.45.42  United Kingdom of Great Britain and Northern    \n",
      "2   3      NaN    142.93.45.42  United Kingdom of Great Britain and Northern    \n",
      "3   4     15.0  154.72.167.173                                       Cameroon   \n",
      "4   5     15.0  154.72.167.173                                       Cameroon   \n",
      "\n",
      "      city                 date   status  \n",
      "0   London  2021-10-17 17:54:27  SUCCESS  \n",
      "1   London  2021-10-17 18:04:57  SUCCESS  \n",
      "2   London  2021-10-18 09:51:12  FAILURE  \n",
      "3  Yaounde  2021-10-20 10:37:22  FAILURE  \n",
      "4  Yaounde  2021-10-20 10:46:17  FAILURE  \n",
      "\n",
      " Aperçu du fichier transfert.csv :\n",
      "   id  operation_id  user_id  receiver_id   amount   status  \\\n",
      "0   1             5       14           16  12500.0  PENDING   \n",
      "1   2             5       23           16     62.0  PENDING   \n",
      "2   3             5       15           16      1.0  PENDING   \n",
      "3   4             5       15           16      1.0  PENDING   \n",
      "4   5             5       15           16      1.0  PENDING   \n",
      "\n",
      "            proceed_at  is_verif ext_id currency  deadline  is_secured  \\\n",
      "0  2021-10-13 07:16:28         0  MLS1T      XAF       NaN           0   \n",
      "1  2021-10-25 17:32:37         0  MLS2T      USD       NaN           0   \n",
      "2  2021-11-01 10:13:48         0  MLS3T      XAF       NaN           0   \n",
      "3  2021-11-01 10:24:31         0  MLS4T      XAF       NaN           0   \n",
      "4  2021-11-01 10:27:28         0  MLS5T      XAF       NaN           0   \n",
      "\n",
      "   password description  \n",
      "0       NaN         NaN  \n",
      "1       NaN         NaN  \n",
      "2       NaN         NaN  \n",
      "3       NaN         NaN  \n",
      "4       NaN         NaN  \n",
      "\n",
      " Aperçu du fichier user.csv :\n",
      "   id    username           date_start       last_operation       balance  \\\n",
      "0   1      bruyan  2020-04-27 00:00:00                  NaN      0.000000   \n",
      "1   6  xpimentOld  2020-05-19 00:00:00  2021-11-18 17:35:54      0.000000   \n",
      "2   8       sopay  2020-08-28 00:00:00  2021-08-06 09:47:36      0.000000   \n",
      "3  14     xpiment  2021-07-13 22:53:03  2025-02-23 01:50:38  16849.663000   \n",
      "4  15      Leonel  2021-07-31 06:36:16  2025-02-14 10:09:16   1082.796194   \n",
      "\n",
      "   level  nb_trans      contact  rewards_balance  default_currency_id  ...  \\\n",
      "0      2       NaN          1.0       292.957322                    1  ...   \n",
      "1      2       4.0   65564106.0         0.270000                    1  ...   \n",
      "2      2      42.0  698479723.0         0.000000                    3  ...   \n",
      "3      1    1635.0  677347922.0        13.992000                    1  ...   \n",
      "4      5     291.0  676411506.0         3.211000                    1  ...   \n",
      "\n",
      "  email_auth_enabled ceiling_credit  score  head  pending_balance  \\\n",
      "0                  0            NaN      0   NaN              NaN   \n",
      "1                  0            NaN      0   NaN              NaN   \n",
      "2                  0            NaN      0   NaN              NaN   \n",
      "3                  0        29.1811    314   NaN              NaN   \n",
      "4                  0         2.2216     21   NaN              NaN   \n",
      "\n",
      "  fees_balance kyc_status       kyc_updated_at kyb_status kyb_updated_at  \n",
      "0          NaN        NaN                  NaN        NaN            NaN  \n",
      "1          NaN        NaN                  NaN        NaN            NaN  \n",
      "2          NaN        NaN                  NaN        NaN            NaN  \n",
      "3          0.0    PENDING  2025-02-17 23:09:02        NaN            NaN  \n",
      "4          NaN        NaN                  NaN        NaN            NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vérifier les premières lignes de chaque fichier\n",
    "print(\"\\n Aperçu du fichier account.csv :\")\n",
    "print(account_df.head())  # Affiche les 5 premières lignes du fichier account.csv\n",
    "\n",
    "print(\"\\n Aperçu du fichier bill.csv :\")\n",
    "print(bill_df.head())  # Affiche les 5 premières lignes du fichier bill.csv\n",
    "\n",
    "print(\"\\n Aperçu du fichier changes.csv :\")\n",
    "print(changes_df.head())  # Affiche les 5 premières lignes du fichier changes.csv\n",
    "\n",
    "print(\"\\n Aperçu du fichier history.csv :\")\n",
    "print(history_df.head())  # Affiche les 5 premières lignes du fichier history.csv\n",
    "\n",
    "print(\"\\n Aperçu du fichier proceed.csv :\")\n",
    "print(proceed_df.head())  # Affiche les 5 premières lignes du fichier proceed.csv\n",
    "\n",
    "print(\"\\n Aperçu du fichier splogin_history.csv :\")\n",
    "print(splogin_history_df.head())  # Affiche les 5 premières lignes du fichier splogin_history.csv\n",
    "\n",
    "print(\"\\n Aperçu du fichier transfert.csv :\")\n",
    "print(transfert_df.head())  # Affiche les 5 premières lignes du fichier transfert.csv\n",
    "\n",
    "print(\"\\n Aperçu du fichier user.csv :\")\n",
    "print(user_df.head())  # Affiche les 5 premières lignes du fichier user.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f16139-720e-48fb-a102-e5b623140889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Structure des datasets :\n",
      "account.csv :  (8112, 8)\n",
      "changes.csv :  (2447, 17)\n",
      "splogin_history.csv :  (1805, 7)\n",
      "transfert.csv :  (168, 14)\n",
      "user.csv :  (2677, 39)\n",
      "proceed.csv :  (10063, 19)\n",
      "history.csv :  (35028, 19)\n",
      "bill.csv :  (20706, 28)\n"
     ]
    }
   ],
   "source": [
    "#nombre de lignes et de colonnes de chaque fichier csv\n",
    "print(\"\\n Structure des datasets :\")\n",
    "print(\"account.csv : \", account_df.shape)\n",
    "print(\"changes.csv : \", changes_df.shape)\n",
    "print(\"splogin_history.csv : \", splogin_history_df.shape)\n",
    "print(\"transfert.csv : \", transfert_df.shape)\n",
    "print(\"user.csv : \", user_df.shape)\n",
    "print(\"proceed.csv : \", proceed_df.shape)\n",
    "print(\"history.csv : \", history_df.shape)\n",
    "print(\"bill.csv : \", bill_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716e0c40-feee-462c-a88e-11d332cdad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Colonnes du fichier account.csv :\n",
      "['id', 'user_id', 'currency_id', 'wallet', 'balance', 'updated_at', 'trade_balance', 'pending_balance']\n",
      "\n",
      "Colonnes du fichier changes.csv :\n",
      "['id', 'in_amount', 'user_id', 'in_wallet', 'out_wallet', 'out_amount', 'status', 'in_service_id', 'out_service_id', 'is_verif', 'ext_id', 'operation_id', 'ext_ref', 'proceed_at', 'other', 'to_account', 'external_reference']\n",
      "\n",
      "Colonnes du fichier splogin_history.csv :\n",
      "['id', 'user_id', 'host', 'country', 'city', 'date', 'status']\n",
      "\n",
      "Colonnes du fichier transfert.csv :\n",
      "['id', 'operation_id', 'user_id', 'receiver_id', 'amount', 'status', 'proceed_at', 'is_verif', 'ext_id', 'currency', 'deadline', 'is_secured', 'password', 'description']\n",
      "\n",
      "Colonnes du fichier user.csv :\n",
      "['id', 'username', 'date_start', 'last_operation', 'balance', 'level', 'nb_trans', 'contact', 'rewards_balance', 'default_currency_id', 'matricule', 'referer', 'verif_phone', 'verif_user', 'total_income_rewards', 'last_login', 'signup_mode', 'random_code', 'last_host_location', 'host_city', 'idd_country', 'active', 'verif_otp', 'is_account_delete', 'is_api_withdrawal', 'login_otp', 'is_api_allow', 'pay_code', 'is_ambassador', 'email_auth_enabled', 'ceiling_credit', 'score', 'head', 'pending_balance', 'fees_balance', 'kyc_status', 'kyc_updated_at', 'kyb_status', 'kyb_updated_at']\n",
      "\n",
      "Colonnes du fichier proceed.csv :\n",
      "['id', 'operation_id', 'service_id', 'user_id', 'ext_id', 'amount', 'wallet', 'currency', 'status', 'description', 'is_verif', 'proceed_at', 'other', 'account_name', 'bank_name', 'otp', 'success_url', 'failure_url', 'external_reference']\n",
      "\n",
      "Colonnes du fichier history.csv :\n",
      "['id', 'document', 'date', 'value', 'status', 'user_id', 'operation_id', 'receiver_matricule', 'account_id', 'ext_id', 'host', 'location', 'city', 'external_reference', 'invoice_sent', 'prev_account_balance', 'current_account_balance', 'transaction_fees', 'completed']\n",
      "\n",
      "Colonnes du fichier bill.csv :\n",
      "['id', 'amount', 'service_id', 'operation_id', 'status', 'user_id', 'currency', 'is_verif', 'ext_id', 'proceed_at', 'wallet', 'other', 'callback', 'description', 'qr_id', 'payer', 'order_id', 'checking', 'send_back', 'updated_at', 'waiting_expired_at', 'success_url', 'failure_url', 'customer_id', 'external_reference', 'payer_email', 'partner_id', 'fees_paid']\n"
     ]
    }
   ],
   "source": [
    "# Affichage des noms de colonnes pour chaque fichier CSV chargé\n",
    "print(\"\\nColonnes du fichier account.csv :\")\n",
    "print(account_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColonnes du fichier changes.csv :\")\n",
    "print(changes_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColonnes du fichier splogin_history.csv :\")\n",
    "print(splogin_history_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColonnes du fichier transfert.csv :\")\n",
    "print(transfert_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColonnes du fichier user.csv :\")\n",
    "print(user_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColonnes du fichier proceed.csv :\")\n",
    "print(proceed_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColonnes du fichier history.csv :\")\n",
    "print(history_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColonnes du fichier bill.csv :\")\n",
    "print(bill_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "647f4c94-cb95-4749-adfa-40de7f5d2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Types de données pour account.csv :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8112 entries, 0 to 8111\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               8112 non-null   int64  \n",
      " 1   user_id          8112 non-null   int64  \n",
      " 2   currency_id      8109 non-null   float64\n",
      " 3   wallet           8066 non-null   object \n",
      " 4   balance          8112 non-null   float64\n",
      " 5   updated_at       8112 non-null   object \n",
      " 6   trade_balance    7146 non-null   float64\n",
      " 7   pending_balance  7138 non-null   float64\n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 507.1+ KB\n",
      "None\n",
      "\n",
      " Types de données pour changes.csv :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2447 entries, 0 to 2446\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  2447 non-null   int64  \n",
      " 1   in_amount           2447 non-null   float64\n",
      " 2   user_id             2447 non-null   int64  \n",
      " 3   in_wallet           2356 non-null   object \n",
      " 4   out_wallet          2356 non-null   object \n",
      " 5   out_amount          2447 non-null   float64\n",
      " 6   status              2447 non-null   object \n",
      " 7   in_service_id       2447 non-null   int64  \n",
      " 8   out_service_id      2447 non-null   int64  \n",
      " 9   is_verif            2447 non-null   int64  \n",
      " 10  ext_id              2439 non-null   object \n",
      " 11  operation_id        2447 non-null   int64  \n",
      " 12  ext_ref             0 non-null      float64\n",
      " 13  proceed_at          2447 non-null   object \n",
      " 14  other               2447 non-null   object \n",
      " 15  to_account          0 non-null      float64\n",
      " 16  external_reference  584 non-null    object \n",
      "dtypes: float64(4), int64(6), object(7)\n",
      "memory usage: 325.1+ KB\n",
      "None\n",
      "\n",
      " Types de données pour splogin_history.csv :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1805 entries, 0 to 1804\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   id       1805 non-null   int64  \n",
      " 1   user_id  1724 non-null   float64\n",
      " 2   host     1805 non-null   object \n",
      " 3   country  1805 non-null   object \n",
      " 4   city     1805 non-null   object \n",
      " 5   date     1805 non-null   object \n",
      " 6   status   1805 non-null   object \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 98.8+ KB\n",
      "None\n",
      "\n",
      " Types de données pour transfert.csv :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 168 entries, 0 to 167\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   id            168 non-null    int64  \n",
      " 1   operation_id  168 non-null    int64  \n",
      " 2   user_id       168 non-null    int64  \n",
      " 3   receiver_id   168 non-null    int64  \n",
      " 4   amount        168 non-null    float64\n",
      " 5   status        168 non-null    object \n",
      " 6   proceed_at    168 non-null    object \n",
      " 7   is_verif      168 non-null    int64  \n",
      " 8   ext_id        168 non-null    object \n",
      " 9   currency      168 non-null    object \n",
      " 10  deadline      0 non-null      float64\n",
      " 11  is_secured    168 non-null    int64  \n",
      " 12  password      0 non-null      float64\n",
      " 13  description   96 non-null     object \n",
      "dtypes: float64(3), int64(6), object(5)\n",
      "memory usage: 18.5+ KB\n",
      "None\n",
      "\n",
      " Types de données pour user.csv :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2677 entries, 0 to 2676\n",
      "Data columns (total 39 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   id                    2677 non-null   int64  \n",
      " 1   username              2677 non-null   object \n",
      " 2   date_start            2677 non-null   object \n",
      " 3   last_operation        386 non-null    object \n",
      " 4   balance               2677 non-null   float64\n",
      " 5   level                 2677 non-null   int64  \n",
      " 6   nb_trans              2676 non-null   float64\n",
      " 7   contact               2653 non-null   float64\n",
      " 8   rewards_balance       2677 non-null   float64\n",
      " 9   default_currency_id   2677 non-null   int64  \n",
      " 10  matricule             1666 non-null   object \n",
      " 11  referer               255 non-null    object \n",
      " 12  verif_phone           2677 non-null   int64  \n",
      " 13  verif_user            2677 non-null   int64  \n",
      " 14  total_income_rewards  277 non-null    float64\n",
      " 15  last_login            1342 non-null   object \n",
      " 16  signup_mode           2578 non-null   object \n",
      " 17  random_code           0 non-null      float64\n",
      " 18  last_host_location    2554 non-null   object \n",
      " 19  host_city             2555 non-null   object \n",
      " 20  idd_country           2572 non-null   object \n",
      " 21  active                2677 non-null   int64  \n",
      " 22  verif_otp             2677 non-null   int64  \n",
      " 23  is_account_delete     2677 non-null   int64  \n",
      " 24  is_api_withdrawal     2677 non-null   int64  \n",
      " 25  login_otp             3 non-null      object \n",
      " 26  is_api_allow          2677 non-null   int64  \n",
      " 27  pay_code              2653 non-null   float64\n",
      " 28  is_ambassador         2677 non-null   int64  \n",
      " 29  email_auth_enabled    2677 non-null   int64  \n",
      " 30  ceiling_credit        1434 non-null   float64\n",
      " 31  score                 2677 non-null   int64  \n",
      " 32  head                  4 non-null      object \n",
      " 33  pending_balance       2 non-null      float64\n",
      " 34  fees_balance          175 non-null    float64\n",
      " 35  kyc_status            19 non-null     object \n",
      " 36  kyc_updated_at        19 non-null     object \n",
      " 37  kyb_status            0 non-null      float64\n",
      " 38  kyb_updated_at        0 non-null      float64\n",
      "dtypes: float64(12), int64(13), object(14)\n",
      "memory usage: 815.8+ KB\n",
      "None\n",
      "\n",
      " Types de données pour proceed.csv :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10063 entries, 0 to 10062\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  10063 non-null  int64  \n",
      " 1   operation_id        10063 non-null  int64  \n",
      " 2   service_id          10063 non-null  int64  \n",
      " 3   user_id             10063 non-null  int64  \n",
      " 4   ext_id              10063 non-null  object \n",
      " 5   amount              10063 non-null  float64\n",
      " 6   wallet              9989 non-null   object \n",
      " 7   currency            10063 non-null  object \n",
      " 8   status              10063 non-null  object \n",
      " 9   description         8999 non-null   object \n",
      " 10  is_verif            10063 non-null  int64  \n",
      " 11  proceed_at          10063 non-null  object \n",
      " 12  other               4392 non-null   object \n",
      " 13  account_name        6 non-null      object \n",
      " 14  bank_name           90 non-null     object \n",
      " 15  otp                 518 non-null    float64\n",
      " 16  success_url         3066 non-null   object \n",
      " 17  failure_url         0 non-null      float64\n",
      " 18  external_reference  5364 non-null   object \n",
      "dtypes: float64(3), int64(5), object(11)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "\n",
      " Types de données pour history.csv :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35028 entries, 0 to 35027\n",
      "Data columns (total 19 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   id                       35028 non-null  object\n",
      " 1   document                 35028 non-null  object\n",
      " 2   date                     35028 non-null  object\n",
      " 3   value                    35028 non-null  object\n",
      " 4   status                   35028 non-null  object\n",
      " 5   user_id                  35028 non-null  object\n",
      " 6   operation_id             35028 non-null  object\n",
      " 7   receiver_matricule       188 non-null    object\n",
      " 8   account_id               34867 non-null  object\n",
      " 9   ext_id                   28187 non-null  object\n",
      " 10  host                     34487 non-null  object\n",
      " 11  location                 34293 non-null  object\n",
      " 12  city                     34293 non-null  object\n",
      " 13  external_reference       295 non-null    object\n",
      " 14  invoice_sent             35028 non-null  object\n",
      " 15  prev_account_balance     20257 non-null  object\n",
      " 16  current_account_balance  20255 non-null  object\n",
      " 17  transaction_fees         15079 non-null  object\n",
      " 18  completed                35028 non-null  object\n",
      "dtypes: object(19)\n",
      "memory usage: 5.1+ MB\n",
      "None\n",
      "\n",
      " Types de données pour bill.csv :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20706 entries, 0 to 20705\n",
      "Data columns (total 28 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  20706 non-null  int64  \n",
      " 1   amount              20706 non-null  float64\n",
      " 2   service_id          20706 non-null  int64  \n",
      " 3   operation_id        20706 non-null  int64  \n",
      " 4   status              20706 non-null  object \n",
      " 5   user_id             20706 non-null  int64  \n",
      " 6   currency            20706 non-null  object \n",
      " 7   is_verif            20706 non-null  int64  \n",
      " 8   ext_id              20706 non-null  object \n",
      " 9   proceed_at          20706 non-null  object \n",
      " 10  wallet              20706 non-null  object \n",
      " 11  other               11867 non-null  object \n",
      " 12  callback            0 non-null      float64\n",
      " 13  description         20449 non-null  object \n",
      " 14  qr_id               2454 non-null   float64\n",
      " 15  payer               13613 non-null  object \n",
      " 16  order_id            18877 non-null  object \n",
      " 17  checking            20706 non-null  int64  \n",
      " 18  send_back           20706 non-null  int64  \n",
      " 19  updated_at          0 non-null      float64\n",
      " 20  waiting_expired_at  0 non-null      float64\n",
      " 21  success_url         16446 non-null  object \n",
      " 22  failure_url         7009 non-null   object \n",
      " 23  customer_id         18489 non-null  float64\n",
      " 24  external_reference  15783 non-null  object \n",
      " 25  payer_email         10920 non-null  object \n",
      " 26  partner_id          0 non-null      float64\n",
      " 27  fees_paid           0 non-null      float64\n",
      "dtypes: float64(8), int64(7), object(13)\n",
      "memory usage: 4.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Vérifier les types de données\n",
    "print(\"\\n Types de données pour account.csv :\")\n",
    "print(account_df.info())  # Afficher les types de données\n",
    "\n",
    "print(\"\\n Types de données pour changes.csv :\")\n",
    "print(changes_df.info())  # Afficher les types de données\n",
    "\n",
    "print(\"\\n Types de données pour splogin_history.csv :\")\n",
    "print(splogin_history_df.info())  # Afficher les types de données\n",
    "\n",
    "print(\"\\n Types de données pour transfert.csv :\")\n",
    "print(transfert_df.info())  # Afficher les types de données\n",
    "\n",
    "print(\"\\n Types de données pour user.csv :\")\n",
    "print(user_df.info())  # Afficher les types de données\n",
    "\n",
    "print(\"\\n Types de données pour proceed.csv :\")\n",
    "print(proceed_df.info())  # Afficher les types de données\n",
    "\n",
    "print(\"\\n Types de données pour history.csv :\")\n",
    "print(history_df.info())  # Afficher les types de données\n",
    "\n",
    "print(\"\\n Types de données pour bill.csv :\")\n",
    "print(bill_df.info())  # Afficher les types de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a4d5b7-d457-4d41-8c10-b818fcd58de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                           int64\n",
      "document                    object\n",
      "date                        object\n",
      "value                      float64\n",
      "status                      object\n",
      "user_id                      int64\n",
      "operation_id                object\n",
      "receiver_matricule          object\n",
      "account_id                 float64\n",
      "ext_id                      object\n",
      "host                        object\n",
      "location                    object\n",
      "city                        object\n",
      "external_reference          object\n",
      "invoice_sent                object\n",
      "prev_account_balance       float64\n",
      "current_account_balance    float64\n",
      "transaction_fees            object\n",
      "completed                   object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#conversion du type de certaines colonne dans history\n",
    "history_df['user_id'] = pd.to_numeric(history_df['user_id'], errors='coerce')\n",
    "history_df['id'] = pd.to_numeric(history_df['id'], errors='coerce')\n",
    "history_df['value'] = pd.to_numeric(history_df['value'], errors='coerce')\n",
    "history_df['account_id'] = pd.to_numeric(history_df['account_id'], errors='coerce')\n",
    "history_df['prev_account_balance'] = pd.to_numeric(history_df['prev_account_balance'], errors='coerce')\n",
    "history_df['current_account_balance'] = pd.to_numeric(history_df['current_account_balance'], errors='coerce')\n",
    "\n",
    "print(history_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b03b7f2-385f-447c-88fd-15e3bf45c63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valeurs manquantes pour account.csv :\n",
      " id                   0\n",
      "user_id              0\n",
      "currency_id          3\n",
      "wallet              46\n",
      "balance              0\n",
      "updated_at           0\n",
      "trade_balance      966\n",
      "pending_balance    974\n",
      "dtype: int64\n",
      "\n",
      "Valeurs manquantes pour changes.csv :\n",
      " id                       0\n",
      "in_amount                0\n",
      "user_id                  0\n",
      "in_wallet               91\n",
      "out_wallet              91\n",
      "out_amount               0\n",
      "status                   0\n",
      "in_service_id            0\n",
      "out_service_id           0\n",
      "is_verif                 0\n",
      "ext_id                   8\n",
      "operation_id             0\n",
      "ext_ref               2447\n",
      "proceed_at               0\n",
      "other                    0\n",
      "to_account            2447\n",
      "external_reference    1863\n",
      "dtype: int64\n",
      "\n",
      "Valeurs manquantes pour transfert.csv :\n",
      " id                0\n",
      "operation_id      0\n",
      "user_id           0\n",
      "receiver_id       0\n",
      "amount            0\n",
      "status            0\n",
      "proceed_at        0\n",
      "is_verif          0\n",
      "ext_id            0\n",
      "currency          0\n",
      "deadline        168\n",
      "is_secured        0\n",
      "password        168\n",
      "description      72\n",
      "dtype: int64\n",
      "\n",
      "Valeurs manquantes pour splogin_history.csv :\n",
      " id          0\n",
      "user_id    81\n",
      "host        0\n",
      "country     0\n",
      "city        0\n",
      "date        0\n",
      "status      0\n",
      "dtype: int64\n",
      "\n",
      "Valeurs manquantes pour user.csv :\n",
      " id                         0\n",
      "username                   0\n",
      "date_start                 0\n",
      "last_operation          2291\n",
      "balance                    0\n",
      "level                      0\n",
      "nb_trans                   1\n",
      "contact                   24\n",
      "rewards_balance            0\n",
      "default_currency_id        0\n",
      "matricule               1011\n",
      "referer                 2422\n",
      "verif_phone                0\n",
      "verif_user                 0\n",
      "total_income_rewards    2400\n",
      "last_login              1335\n",
      "signup_mode               99\n",
      "random_code             2677\n",
      "last_host_location       123\n",
      "host_city                122\n",
      "idd_country              105\n",
      "active                     0\n",
      "verif_otp                  0\n",
      "is_account_delete          0\n",
      "is_api_withdrawal          0\n",
      "login_otp               2674\n",
      "is_api_allow               0\n",
      "pay_code                  24\n",
      "is_ambassador              0\n",
      "email_auth_enabled         0\n",
      "ceiling_credit          1243\n",
      "score                      0\n",
      "head                    2673\n",
      "pending_balance         2675\n",
      "fees_balance            2502\n",
      "kyc_status              2658\n",
      "kyc_updated_at          2658\n",
      "kyb_status              2677\n",
      "kyb_updated_at          2677\n",
      "dtype: int64\n",
      "\n",
      "Valeurs manquantes pour proceed.csv :\n",
      " id                        0\n",
      "operation_id              0\n",
      "service_id                0\n",
      "user_id                   0\n",
      "ext_id                    0\n",
      "amount                    0\n",
      "wallet                   74\n",
      "currency                  0\n",
      "status                    0\n",
      "description            1064\n",
      "is_verif                  0\n",
      "proceed_at                0\n",
      "other                  5671\n",
      "account_name          10057\n",
      "bank_name              9973\n",
      "otp                    9545\n",
      "success_url            6997\n",
      "failure_url           10063\n",
      "external_reference     4699\n",
      "dtype: int64\n",
      "\n",
      "Valeurs manquantes pour history.csv :\n",
      " id                             0\n",
      "document                       0\n",
      "date                           0\n",
      "value                          0\n",
      "status                         0\n",
      "user_id                        0\n",
      "operation_id                   0\n",
      "receiver_matricule         34840\n",
      "account_id                   161\n",
      "ext_id                      6841\n",
      "host                         541\n",
      "location                     735\n",
      "city                         735\n",
      "external_reference         34733\n",
      "invoice_sent                   0\n",
      "prev_account_balance       14771\n",
      "current_account_balance    14773\n",
      "transaction_fees           19949\n",
      "completed                      0\n",
      "dtype: int64\n",
      "\n",
      "Valeurs manquantes pour bill.csv :\n",
      " id                        0\n",
      "amount                    0\n",
      "service_id                0\n",
      "operation_id              0\n",
      "status                    0\n",
      "user_id                   0\n",
      "currency                  0\n",
      "is_verif                  0\n",
      "ext_id                    0\n",
      "proceed_at                0\n",
      "wallet                    0\n",
      "other                  8839\n",
      "callback              20706\n",
      "description             257\n",
      "qr_id                 18252\n",
      "payer                  7093\n",
      "order_id               1829\n",
      "checking                  0\n",
      "send_back                 0\n",
      "updated_at            20706\n",
      "waiting_expired_at    20706\n",
      "success_url            4260\n",
      "failure_url           13697\n",
      "customer_id            2217\n",
      "external_reference     4923\n",
      "payer_email            9786\n",
      "partner_id            20706\n",
      "fees_paid             20706\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#examiner les valeurs manquantes\n",
    "print(\"\\nValeurs manquantes pour account.csv :\\n\", account_df.isnull().sum())\n",
    "print(\"\\nValeurs manquantes pour changes.csv :\\n\", changes_df.isnull().sum())\n",
    "print(\"\\nValeurs manquantes pour transfert.csv :\\n\", transfert_df.isnull().sum())\n",
    "print(\"\\nValeurs manquantes pour splogin_history.csv :\\n\", splogin_history_df.isnull().sum())\n",
    "print(\"\\nValeurs manquantes pour user.csv :\\n\", user_df.isnull().sum())\n",
    "print(\"\\nValeurs manquantes pour proceed.csv :\\n\", proceed_df.isnull().sum())\n",
    "print(\"\\nValeurs manquantes pour history.csv :\\n\", history_df.isnull().sum())\n",
    "print(\"\\nValeurs manquantes pour bill.csv :\\n\", bill_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b60e14-0766-42a1-8c5a-d11b3f65d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes inutiles supprimees avec succes !\n"
     ]
    }
   ],
   "source": [
    "#suppression  des colonnes inutiles pour chaque fichiers\n",
    "account_df.drop(columns=[\"pending_balance\"], inplace=True)\n",
    "changes_df.drop(columns=[\"to_account\", \"ext_ref\"], inplace=True)\n",
    "splogin_history_df.drop(columns=[\"date\"], inplace=True)\n",
    "transfert_df.drop(columns=[\"deadline\", \"password\", \"description\"], inplace=True)\n",
    "user_df.drop(columns=[\"default_currency_id\", \"referer\", \"level\", \"total_income_rewards\", \"random_code\", \"login_otp\", \"pay_code\", \"is_ambassador\", \"ceiling_credit\", \"score\", \"head\", \"pending_balance\", \"fees_balance\", \"kyc_updated_at\", \"kyb_status\", \"kyb_updated_at\"], inplace=True)\n",
    "proceed_df.drop(columns=[\"description\", \"otp\", \"success_url\", \"failure_url\", \"bank_name\", \"account_name\"], inplace=True)\n",
    "history_df.drop(columns=[\"invoice_sent\", \"completed\", \"transaction_fees\", \"receiver_matricule\", \"external_reference\"], inplace=True)\n",
    "bill_df.drop(columns=[\"callback\", \"description\", \"checking\", \"send_back\", \"updated_at\", \"waiting_expired_at\", \"success_url\", \"failure_url\", \"partner_id\", \"fees_paid\"], inplace=True)  #colonnes inutile pour le fichier bill.csv\n",
    "\n",
    "print(\"Colonnes inutiles supprimees avec succes !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e35b743-a632-4978-849f-c163927e44b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suppression des doublons terminée !\n"
     ]
    }
   ],
   "source": [
    "#suppression des doublons\n",
    "account_df.drop_duplicates(inplace=True)\n",
    "changes_df.drop_duplicates(inplace=True)\n",
    "splogin_history_df.drop_duplicates(inplace=True)\n",
    "transfert_df.drop_duplicates(inplace=True)\n",
    "user_df.drop_duplicates(inplace=True)\n",
    "proceed_df.drop_duplicates(inplace=True)\n",
    "bill_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"Suppression des doublons terminée !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d55dad8-2e55-42ee-8f91-b4274b4db578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Colonnes restantes dans account.csv : ['id', 'user_id', 'currency_id', 'wallet', 'balance', 'updated_at', 'trade_balance']\n",
      "\n",
      "Colonnes restantes dans changes.csv : ['id', 'in_amount', 'user_id', 'in_wallet', 'out_wallet', 'out_amount', 'status', 'in_service_id', 'out_service_id', 'is_verif', 'ext_id', 'operation_id', 'proceed_at', 'other', 'external_reference']\n",
      "\n",
      "Colonnes restantes dans splogin_history.csv : ['id', 'user_id', 'host', 'country', 'city', 'status']\n",
      "\n",
      "Colonnes restantes dans transfert.csv : ['id', 'operation_id', 'user_id', 'receiver_id', 'amount', 'status', 'proceed_at', 'is_verif', 'ext_id', 'currency', 'is_secured']\n",
      "\n",
      "Colonnes restantes dans user.csv : ['id', 'username', 'date_start', 'last_operation', 'balance', 'nb_trans', 'contact', 'rewards_balance', 'matricule', 'verif_phone', 'verif_user', 'last_login', 'signup_mode', 'last_host_location', 'host_city', 'idd_country', 'active', 'verif_otp', 'is_account_delete', 'is_api_withdrawal', 'is_api_allow', 'email_auth_enabled', 'kyc_status']\n",
      "\n",
      "Colonnes restantes dans proceed.csv : ['id', 'operation_id', 'service_id', 'user_id', 'ext_id', 'amount', 'wallet', 'currency', 'status', 'is_verif', 'proceed_at', 'other', 'external_reference']\n",
      "\n",
      "Colonnes restantes dans history.csv : ['id', 'document', 'date', 'value', 'status', 'user_id', 'operation_id', 'account_id', 'ext_id', 'host', 'location', 'city', 'prev_account_balance', 'current_account_balance']\n",
      "\n",
      "Colonnes restantes dans bill.csv : ['id', 'amount', 'service_id', 'operation_id', 'status', 'user_id', 'currency', 'is_verif', 'ext_id', 'proceed_at', 'wallet', 'other', 'qr_id', 'payer', 'order_id', 'customer_id', 'external_reference', 'payer_email']\n"
     ]
    }
   ],
   "source": [
    "# Affichage des colonnes restantes après suppression\n",
    "print(\"\\nColonnes restantes dans account.csv :\", account_df.columns.tolist())\n",
    "print(\"\\nColonnes restantes dans changes.csv :\", changes_df.columns.tolist())\n",
    "print(\"\\nColonnes restantes dans splogin_history.csv :\", splogin_history_df.columns.tolist())\n",
    "print(\"\\nColonnes restantes dans transfert.csv :\", transfert_df.columns.tolist())\n",
    "print(\"\\nColonnes restantes dans user.csv :\", user_df.columns.tolist())\n",
    "print(\"\\nColonnes restantes dans proceed.csv :\", proceed_df.columns.tolist())\n",
    "print(\"\\nColonnes restantes dans history.csv :\", history_df.columns.tolist())\n",
    "print(\"\\nColonnes restantes dans bill.csv :\", bill_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "248f09df-e427-4682-8c96-6ecd15c5b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les valeurs manquantes ont bien été remplacées\n"
     ]
    }
   ],
   "source": [
    "#gestion des valeurs manquantes numeriques et categoriques\n",
    "\n",
    "#1- account.csv\n",
    "account_df.loc[:, 'currency_id'] = account_df['currency_id'].fillna(account_df['currency_id'].mode()[0]) #remplacement de la colonne par la devise la plus frequente\n",
    "account_df.loc[:, 'wallet'] = account_df['wallet'].fillna(\"INACTIVE\") # remplacement de la colonne par INACTIVE pour portefeuille non actif\n",
    "account_df.loc[:, 'trade_balance'] = account_df['trade_balance'].fillna(0) #remplacement de la colonne par 0 pour pas d'argent converti\n",
    "\n",
    "#2- changes.csv\n",
    "changes_df.loc[:, 'in_wallet'] = changes_df['in_wallet'].fillna(\"INACTIVE\") #indique portefeuil non actif (pas d'entree)\n",
    "changes_df.loc[:, 'out_wallet'] = changes_df['out_wallet'].fillna(\"INACTIVE\") # indique un portefeuille non actif (pas de sortie)\n",
    "changes_df.loc[:, 'external_reference'] = changes_df['external_reference'].fillna(\"UNKNOWN\") #indique la reference externe inconnu\n",
    "\n",
    "#3- splogin_history.csv\n",
    "splogin_history_df.loc[:, 'user_id'] = splogin_history_df['user_id'].fillna(splogin_history_df['user_id'].mode()[0]) #remplacement par l'identifiant de l'utilisateur le plus frequent\n",
    "\n",
    "#4-transfert.csv\n",
    "\n",
    "#5- user.csv\n",
    "user_df.loc[:, 'last_operation'] = user_df['last_operation'].fillna(\"UNKNOWN\")#aucune operation enregistree\n",
    "user_df.loc[:, 'nb_trans'] = user_df['nb_trans'].fillna(0)#pas de transaction effectuee\n",
    "user_df.loc[:, 'contact'] = user_df['contact'].fillna(0)#contact inconnu\n",
    "user_df.loc[:, 'matricule'] = user_df['matricule'].fillna(\"UNKNOWN\")#numero inconnu \n",
    "user_df.loc[:, 'last_login'] = user_df['last_login'].fillna(\"NEVER\")#pas de connexion\n",
    "user_df.loc[:, 'signup_mode'] = user_df['signup_mode'].fillna(\"UNKNOWN\")#pour type de compte inconnu\n",
    "user_df.loc[:, 'last_host_location'] = user_df['last_host_location'].fillna(\"UNKNOWN\")#derniere localisation inconnue\n",
    "user_df.loc[:, 'host_city'] = user_df['host_city'].fillna(\"UNKNOWN\")#ville inconnue\n",
    "user_df.loc[:, 'idd_country'] = user_df['idd_country'].fillna(\"UNKNOWN\")#pays inconnue\n",
    "user_df.loc[:, 'kyc_status'] = user_df['kyc_status'].fillna(\"NOT_INITIATED\")#kyc pas initie\n",
    "\n",
    "#6- proceed.csv\n",
    "proceed_df.loc[:, 'wallet'] = proceed_df['wallet'].fillna(\"INACTIVE\")#portefeuil non actif\n",
    "proceed_df.loc[:, 'other'] = proceed_df['other'].fillna(\"UNKNOWN\")#informations supplementaires inconnues\n",
    "proceed_df.loc[:, 'external_reference'] = proceed_df['external_reference'].fillna(\"UNKNOWN\")#reference externe manquante\n",
    "\n",
    "#7- history\n",
    "history_df.loc[:, 'account_id'] = history_df['account_id'].fillna(-1)#compte inconnue\n",
    "history_df.loc[:, 'ext_id'] = history_df['ext_id'].fillna(\"UNKNOWN\")#identifiant externe inconnue\n",
    "history_df.loc[:, 'host'] = history_df['host'].fillna(\"UNKNOWN\")#adresse ip inconnue\n",
    "history_df.loc[:, 'location'] = history_df['location'].fillna(\"UNKNOWN\")#localisation inconnue\n",
    "history_df.loc[:, 'prev_account_balance'] = history_df['prev_account_balance'].fillna(0)#remplacement par 0 pour solde precedent manquant\n",
    "history_df.loc[:, 'current_account_balance'] = history_df['current_account_balance'].fillna(0)#remplacement par 0 pour solde actuel manquant\n",
    "\n",
    "#8- bill.csv (manque customer_id)\n",
    "bill_df.loc[:, 'other'] = bill_df['other'].fillna('UNKNOWN')#informations supplementaires inconnue\n",
    "bill_df.loc[:, 'qr_id'] = bill_df['qr_id'].fillna(-1)#pour QR code manquant\n",
    "bill_df.loc[:, 'payer'] = bill_df['payer'].fillna('UNKNOWN')#informations inconnues sur le payeur\n",
    "bill_df.loc[:, 'order_id'] = bill_df['order_id'].fillna('UNKNOWN')#identifiant de la commande inconnue\n",
    "bill_df.loc[:, 'external_reference'] = bill_df['external_reference'].fillna('UNKNOWN')#reference externe inconnue\n",
    "bill_df.loc[:, 'payer_email'] = bill_df['payer_email'].fillna('UNKNOWN')#email du payeur inconnu\n",
    "\n",
    "print(\"Les valeurs manquantes ont bien été remplacées\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5030d55-efad-4632-8686-eaee1711e5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification des valeurs manquantes apres traitement:\n",
      "\n",
      "account.csv:\n",
      " id               0\n",
      "user_id          0\n",
      "currency_id      0\n",
      "wallet           0\n",
      "balance          0\n",
      "updated_at       0\n",
      "trade_balance    0\n",
      "dtype: int64\n",
      "\n",
      "changes.csv :\n",
      " id                    0\n",
      "in_amount             0\n",
      "user_id               0\n",
      "in_wallet             0\n",
      "out_wallet            0\n",
      "out_amount            0\n",
      "status                0\n",
      "in_service_id         0\n",
      "out_service_id        0\n",
      "is_verif              0\n",
      "ext_id                8\n",
      "operation_id          0\n",
      "proceed_at            0\n",
      "other                 0\n",
      "external_reference    0\n",
      "dtype: int64\n",
      "\n",
      "splogin_history.csv :\n",
      " id         0\n",
      "user_id    0\n",
      "host       0\n",
      "country    0\n",
      "city       0\n",
      "status     0\n",
      "dtype: int64\n",
      "\n",
      "transfert.csv :\n",
      " id              0\n",
      "operation_id    0\n",
      "user_id         0\n",
      "receiver_id     0\n",
      "amount          0\n",
      "status          0\n",
      "proceed_at      0\n",
      "is_verif        0\n",
      "ext_id          0\n",
      "currency        0\n",
      "is_secured      0\n",
      "dtype: int64\n",
      "\n",
      "user.csv :\n",
      " id                    0\n",
      "username              0\n",
      "date_start            0\n",
      "last_operation        0\n",
      "balance               0\n",
      "nb_trans              0\n",
      "contact               0\n",
      "rewards_balance       0\n",
      "matricule             0\n",
      "verif_phone           0\n",
      "verif_user            0\n",
      "last_login            0\n",
      "signup_mode           0\n",
      "last_host_location    0\n",
      "host_city             0\n",
      "idd_country           0\n",
      "active                0\n",
      "verif_otp             0\n",
      "is_account_delete     0\n",
      "is_api_withdrawal     0\n",
      "is_api_allow          0\n",
      "email_auth_enabled    0\n",
      "kyc_status            0\n",
      "dtype: int64\n",
      "\n",
      "proceed.csv :\n",
      " id                    0\n",
      "operation_id          0\n",
      "service_id            0\n",
      "user_id               0\n",
      "ext_id                0\n",
      "amount                0\n",
      "wallet                0\n",
      "currency              0\n",
      "status                0\n",
      "is_verif              0\n",
      "proceed_at            0\n",
      "other                 0\n",
      "external_reference    0\n",
      "dtype: int64\n",
      "\n",
      "history.csv :\n",
      " id                           0\n",
      "document                     0\n",
      "date                         0\n",
      "value                        0\n",
      "status                       0\n",
      "user_id                      0\n",
      "operation_id                 0\n",
      "account_id                   0\n",
      "ext_id                       0\n",
      "host                         0\n",
      "location                     0\n",
      "city                       735\n",
      "prev_account_balance         0\n",
      "current_account_balance      0\n",
      "dtype: int64\n",
      "\n",
      "bill.csv :\n",
      " id                       0\n",
      "amount                   0\n",
      "service_id               0\n",
      "operation_id             0\n",
      "status                   0\n",
      "user_id                  0\n",
      "currency                 0\n",
      "is_verif                 0\n",
      "ext_id                   0\n",
      "proceed_at               0\n",
      "wallet                   0\n",
      "other                    0\n",
      "qr_id                    0\n",
      "payer                    0\n",
      "order_id                 0\n",
      "customer_id           2217\n",
      "external_reference       0\n",
      "payer_email              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#verification des valeurs manquantes apres traitement\n",
    "print(\"\\nVerification des valeurs manquantes apres traitement:\")\n",
    "print(\"\\naccount.csv:\\n\", account_df.isnull().sum())\n",
    "print(\"\\nchanges.csv :\\n\", changes_df.isnull().sum())\n",
    "print(\"\\nsplogin_history.csv :\\n\", splogin_history_df.isnull().sum()) \n",
    "print(\"\\ntransfert.csv :\\n\", transfert_df.isnull().sum()) \n",
    "print(\"\\nuser.csv :\\n\", user_df.isnull().sum()) \n",
    "print(\"\\nproceed.csv :\\n\", proceed_df.isnull().sum())\n",
    "print(\"\\nhistory.csv :\\n\", history_df.isnull().sum()) \n",
    "print(\"\\nbill.csv :\\n\", bill_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "983de252-5749-4774-96d4-6b46707223c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->Colonnes categoriques pour account.csv : Index(['wallet', 'updated_at'], dtype='object')\n",
      "wallet: 4952 valeurs uniques\n",
      "updated_at: 1953 valeurs uniques\n",
      "-->Colonnes categoriques changes.csv: Index(['in_wallet', 'out_wallet', 'status', 'ext_id', 'proceed_at', 'other',\n",
      "       'external_reference'],\n",
      "      dtype='object')\n",
      "in_wallet: 174 valeurs uniques\n",
      "out_wallet: 183 valeurs uniques\n",
      "status: 4 valeurs uniques\n",
      "ext_id: 2439 valeurs uniques\n",
      "proceed_at: 2434 valeurs uniques\n",
      "other: 2447 valeurs uniques\n",
      "external_reference: 585 valeurs uniques\n",
      "-->Colonnes categoriques sploging_history.csv: Index(['host', 'country', 'city', 'status'], dtype='object')\n",
      "host: 635 valeurs uniques\n",
      "country: 20 valeurs uniques\n",
      "city: 87 valeurs uniques\n",
      "status: 2 valeurs uniques\n",
      "-->Colonnes categoriques tansfert.csv: Index(['status', 'proceed_at', 'ext_id', 'currency'], dtype='object')\n",
      "status: 2 valeurs uniques\n",
      "proceed_at: 164 valeurs uniques\n",
      "ext_id: 168 valeurs uniques\n",
      "currency: 3 valeurs uniques\n",
      "-->Colonnes categoriques user.csv: Index(['username', 'date_start', 'last_operation', 'matricule', 'last_login',\n",
      "       'signup_mode', 'last_host_location', 'host_city', 'idd_country',\n",
      "       'kyc_status'],\n",
      "      dtype='object')\n",
      "username: 2569 valeurs uniques\n",
      "date_start: 2676 valeurs uniques\n",
      "last_operation: 385 valeurs uniques\n",
      "matricule: 1667 valeurs uniques\n",
      "last_login: 1342 valeurs uniques\n",
      "signup_mode: 4 valeurs uniques\n",
      "last_host_location: 44 valeurs uniques\n",
      "host_city: 179 valeurs uniques\n",
      "idd_country: 70 valeurs uniques\n",
      "kyc_status: 4 valeurs uniques\n",
      "-->Colonnes categoriques proceed.csv: Index(['ext_id', 'wallet', 'currency', 'status', 'proceed_at', 'other',\n",
      "       'external_reference'],\n",
      "      dtype='object')\n",
      "ext_id: 10058 valeurs uniques\n",
      "wallet: 2076 valeurs uniques\n",
      "currency: 7 valeurs uniques\n",
      "status: 5 valeurs uniques\n",
      "proceed_at: 10033 valeurs uniques\n",
      "other: 4102 valeurs uniques\n",
      "external_reference: 5273 valeurs uniques\n",
      "-->Colonnes categoriques history.csv: Index(['document', 'date', 'status', 'operation_id', 'ext_id', 'host',\n",
      "       'location', 'city'],\n",
      "      dtype='object')\n",
      "document: 34496 valeurs uniques\n",
      "date: 34553 valeurs uniques\n",
      "status: 9 valeurs uniques\n",
      "operation_id: 14 valeurs uniques\n",
      "ext_id: 23473 valeurs uniques\n",
      "host: 4406 valeurs uniques\n",
      "location: 58 valeurs uniques\n",
      "city: 278 valeurs uniques\n",
      "-->Colonnes categoriques bill.csv: Index(['status', 'currency', 'ext_id', 'proceed_at', 'wallet', 'other',\n",
      "       'payer', 'order_id', 'external_reference', 'payer_email'],\n",
      "      dtype='object')\n",
      "status: 6 valeurs uniques\n",
      "currency: 9 valeurs uniques\n",
      "ext_id: 20706 valeurs uniques\n",
      "proceed_at: 20609 valeurs uniques\n",
      "wallet: 7154 valeurs uniques\n",
      "other: 9595 valeurs uniques\n",
      "payer: 5104 valeurs uniques\n",
      "order_id: 15633 valeurs uniques\n",
      "external_reference: 15784 valeurs uniques\n",
      "payer_email: 2818 valeurs uniques\n"
     ]
    }
   ],
   "source": [
    "#encodage des variables categoriques\n",
    "\n",
    "#1- detection des colonnes categoriques et determination de l'encodage a utiliser\n",
    "categorical_columns = account_df.select_dtypes(include=['object']).columns #detection colonne categorique\n",
    "print(\"-->Colonnes categoriques pour account.csv :\", categorical_columns)\n",
    "for col in categorical_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {account_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_columns = changes_df.select_dtypes(include=['object']).columns\n",
    "print(\"-->Colonnes categoriques changes.csv:\", categorical_columns)\n",
    "for col in categorical_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {changes_df[col].nunique()} valeurs uniques\")\n",
    "    \n",
    "categorical_columns = splogin_history_df.select_dtypes(include=['object']).columns\n",
    "print(\"-->Colonnes categoriques sploging_history.csv:\", categorical_columns)\n",
    "for col in categorical_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {splogin_history_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_columns = transfert_df.select_dtypes(include=['object']).columns\n",
    "print(\"-->Colonnes categoriques tansfert.csv:\", categorical_columns)\n",
    "for col in categorical_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {transfert_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_columns = user_df.select_dtypes(include=['object']).columns\n",
    "print(\"-->Colonnes categoriques user.csv:\", categorical_columns)\n",
    "for col in categorical_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {user_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_columns = proceed_df.select_dtypes(include=['object']).columns\n",
    "print(\"-->Colonnes categoriques proceed.csv:\", categorical_columns)\n",
    "for col in categorical_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {proceed_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_columns = history_df.select_dtypes(include=['object']).columns\n",
    "print(\"-->Colonnes categoriques history.csv:\", categorical_columns)\n",
    "for col in categorical_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {history_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_columns = bill_df.select_dtypes(include=['object']).columns\n",
    "print(\"-->Colonnes categoriques bill.csv:\", categorical_columns)\n",
    "for col in categorical_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {bill_df[col].nunique()} valeurs uniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88ba8f2b-9d28-40e8-8655-e8d2fe9d3552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->Colonnes numeriques qui sont Categorique pour account.csv: ['currency_id']\n",
      "currency_id: 10 valeurs uniques\n",
      "-->Colonnes numeriques qui sont Categorique pour changes.csv: ['is_verif', 'operation_id']\n",
      "is_verif: 2 valeurs uniques\n",
      "operation_id: 3 valeurs uniques\n",
      "-->Colonnes numeriques qui sont Categorique pour splogin_history.csv: []\n",
      "-->Colonnes numeriques qui sont Categorique pour transfert.csv: ['operation_id', 'is_verif', 'is_secured']\n",
      "operation_id: 1 valeurs uniques\n",
      "is_verif: 2 valeurs uniques\n",
      "is_secured: 1 valeurs uniques\n",
      "-->Colonnes numeriques qui sont Categorique pour user.csv: ['verif_phone', 'verif_user', 'active', 'verif_otp', 'is_account_delete', 'is_api_withdrawal', 'is_api_allow', 'email_auth_enabled']\n",
      "verif_phone: 2 valeurs uniques\n",
      "verif_user: 2 valeurs uniques\n",
      "active: 2 valeurs uniques\n",
      "verif_otp: 2 valeurs uniques\n",
      "is_account_delete: 2 valeurs uniques\n",
      "is_api_withdrawal: 2 valeurs uniques\n",
      "is_api_allow: 2 valeurs uniques\n",
      "email_auth_enabled: 2 valeurs uniques\n",
      "-->Colonnes numeriques qui sont Categorique pour proceed.csv: ['operation_id', 'is_verif']\n",
      "operation_id: 2 valeurs uniques\n",
      "is_verif: 3 valeurs uniques\n",
      "-->Colonnes numeriques qui sont Categorique pour history.csv: []\n",
      "-->Colonnes numeriques qui sont Categorique pour bill.csv: ['operation_id', 'is_verif']\n",
      "operation_id: 1 valeurs uniques\n",
      "is_verif: 2 valeurs uniques\n"
     ]
    }
   ],
   "source": [
    "#2- detection des colonnes numeriques qui peuvent etre categorique et determination de l'encodage a utiliser\n",
    "\n",
    "categorical_numeric_columns = [col for col in account_df.select_dtypes(include=['int64', 'float64']).columns if account_df[col].nunique() < 20]\n",
    "print(\"-->Colonnes numeriques qui sont Categorique pour account.csv:\", categorical_numeric_columns)\n",
    "for col in categorical_numeric_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {account_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_numeric_columns = [col for col in changes_df.select_dtypes(include=['int64', 'float64']).columns if changes_df[col].nunique() < 20]\n",
    "print(\"-->Colonnes numeriques qui sont Categorique pour changes.csv:\", categorical_numeric_columns)\n",
    "for col in categorical_numeric_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {changes_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_numeric_columns = [col for col in splogin_history_df.select_dtypes(include=['int64', 'float64']).columns if splogin_history_df[col].nunique() < 20]\n",
    "print(\"-->Colonnes numeriques qui sont Categorique pour splogin_history.csv:\", categorical_numeric_columns)\n",
    "for col in categorical_numeric_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {sploging_history_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_numeric_columns = [col for col in transfert_df.select_dtypes(include=['int64', 'float64']).columns if transfert_df[col].nunique() < 20]\n",
    "print(\"-->Colonnes numeriques qui sont Categorique pour transfert.csv:\", categorical_numeric_columns)\n",
    "for col in categorical_numeric_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {transfert_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_numeric_columns = [col for col in user_df.select_dtypes(include=['int64', 'float64']).columns if user_df[col].nunique() < 20]\n",
    "print(\"-->Colonnes numeriques qui sont Categorique pour user.csv:\", categorical_numeric_columns)\n",
    "for col in categorical_numeric_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {user_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_numeric_columns = [col for col in proceed_df.select_dtypes(include=['int64', 'float64']).columns if proceed_df[col].nunique() < 20]\n",
    "print(\"-->Colonnes numeriques qui sont Categorique pour proceed.csv:\", categorical_numeric_columns)\n",
    "for col in categorical_numeric_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {proceed_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_numeric_columns = [col for col in history_df.select_dtypes(include=['int64', 'float64']).columns if history_df[col].nunique() < 20]\n",
    "print(\"-->Colonnes numeriques qui sont Categorique pour history.csv:\", categorical_numeric_columns)\n",
    "for col in categorical_numeric_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {history_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "categorical_numeric_columns = [col for col in bill_df.select_dtypes(include=['int64', 'float64']).columns if bill_df[col].nunique() < 20]\n",
    "print(\"-->Colonnes numeriques qui sont Categorique pour bill.csv:\", categorical_numeric_columns)\n",
    "for col in categorical_numeric_columns: #verification des valeurs uniques pour determiner l'encodage\n",
    "    print(f\"{col}: {bill_df[col].nunique()} valeurs uniques\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b264e4-91dc-4d7a-81c3-fc8e649bb727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodage frequentiel terminee\n"
     ]
    }
   ],
   "source": [
    "#3-utilisation de l'encodage approprie pour chaque categorie determinee\n",
    "#3-1) encodage frequentiel\n",
    "    #a) account.csv\n",
    "account_df['wallet'] = account_df['wallet'].map(account_df['wallet'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "account_df['updated_at'] = account_df['updated_at'].map(account_df['updated_at'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "\n",
    "    #b) changes.csv\n",
    "changes_df['in_wallet'] = changes_df['in_wallet'].map(changes_df['in_wallet'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "changes_df['out_wallet'] = changes_df['out_wallet'].map(changes_df['out_wallet'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "changes_df['ext_id'] = changes_df['ext_id'].map(changes_df['ext_id'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "changes_df['proceed_at'] = changes_df['proceed_at'].map(changes_df['proceed_at'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "changes_df['other'] = changes_df['other'].map(changes_df['other'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "changes_df['external_reference'] = changes_df['external_reference'].map(changes_df['external_reference'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "\n",
    "    #c)sploging_history.csv\n",
    "splogin_history_df['host'] = splogin_history_df['host'].map(splogin_history_df['host'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "\n",
    "    #d)transfert.csv\n",
    "transfert_df['proceed_at'] = transfert_df['proceed_at'].map(transfert_df['proceed_at'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "transfert_df['ext_id'] = transfert_df['ext_id'].map(transfert_df['ext_id'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "\n",
    "    #e)user\n",
    "user_df['username'] = user_df['username'].map(user_df['username'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "user_df['date_start'] = user_df['date_start'].map(user_df['date_start'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "user_df['last_operation'] = user_df['last_operation'].map(user_df['last_operation'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "user_df['contact'] = user_df['contact'].map(user_df['contact'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "user_df['matricule'] = user_df['matricule'].map(user_df['matricule'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "user_df['last_login'] = user_df['last_login'].map(user_df['last_login'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "user_df['host_city'] = user_df['host_city'].map(user_df['host_city'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "\n",
    "    #f)proceed\n",
    "proceed_df['ext_id'] = proceed_df['ext_id'].map(proceed_df['ext_id'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "proceed_df['wallet'] = proceed_df['wallet'].map(proceed_df['wallet'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "proceed_df['proceed_at'] = proceed_df['proceed_at'].map(proceed_df['proceed_at'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "proceed_df['other'] = proceed_df['other'].map(proceed_df['other'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "proceed_df['external_reference'] = proceed_df['external_reference'].map(proceed_df['external_reference'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "\n",
    "    #g)history\n",
    "history_df['document'] = history_df['document'].map(history_df['document'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "history_df['date'] = history_df['date'].map(history_df['date'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "history_df['ext_id'] = history_df['ext_id'].map(history_df['ext_id'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "history_df['host'] = history_df['host'].map(history_df['host'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "history_df['city'] = history_df['city'].map(history_df['city'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "\n",
    "    #h)bill\n",
    "bill_df['ext_id'] = bill_df['ext_id'].map(bill_df['ext_id'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "bill_df['proceed_at'] = bill_df['proceed_at'].map(bill_df['proceed_at'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "bill_df['wallet'] = bill_df['wallet'].map(bill_df['wallet'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "bill_df['other'] = bill_df['other'].map(bill_df['other'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "bill_df['payer'] = bill_df['payer'].map(bill_df['payer'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "bill_df['order_id'] = bill_df['order_id'].map(bill_df['order_id'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "bill_df['external_reference'] = bill_df['external_reference'].map(bill_df['external_reference'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "bill_df['payer_email'] = bill_df['payer_email'].map(bill_df['payer_email'].value_counts(normalize=True))#encodage frequentiel car valeurs unique>100\n",
    "\n",
    "print(\"Encodage frequentiel terminee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2dcc8f6-962f-4b4c-b434-7868cc04d4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodage binaire termernier avec success!\n"
     ]
    }
   ],
   "source": [
    "#3-2) One-Hot encoding(encodage binaire)\n",
    "#importation de la bibliotheque scikit-learn pour le machine learning\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "    #a)account csv\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(account_df[['currency_id']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['currency_id'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "account_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "account_df_encoded.index = account_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "account_df = pd.concat([account_df, account_df_encoded], axis=1).drop(columns=['currency_id'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "    #b)changes.csv\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(changes_df[['status']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['status'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "changes_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "changes_df_encoded.index = changes_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "changes_df = pd.concat([changes_df, changes_df_encoded], axis=1).drop(columns=['status'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(changes_df[['is_verif']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['is_verif'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "changes_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "changes_df_encoded.index = changes_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "changes_df = pd.concat([changes_df, changes_df_encoded], axis=1).drop(columns=['is_verif'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(changes_df[['operation_id']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['operation_id'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "changes_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "changes_df_encoded.index = changes_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "changes_df = pd.concat([changes_df, changes_df_encoded], axis=1).drop(columns=['operation_id'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "    #c)splogin_history\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(splogin_history_df[['status']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['status'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "splogin_history_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "splogin_history_df_encoded.index = splogin_history_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "splogin_history_df = pd.concat([splogin_history_df, splogin_history_df_encoded], axis=1).drop(columns=['status'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "    #d)transfert\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(transfert_df[['status']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['status'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "transfert_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "transfert_df_encoded.index = transfert_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "transfert_df = pd.concat([transfert_df, transfert_df_encoded], axis=1).drop(columns=['status'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(transfert_df[['currency']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['currency'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "transfert_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "transfert_df_encoded.index = transfert_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "transfert_df = pd.concat([transfert_df, transfert_df_encoded], axis=1).drop(columns=['currency'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(transfert_df[['operation_id']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['operation_id'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "transfert_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "transfert_df_encoded.index = transfert_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "transfert_df = pd.concat([transfert_df, transfert_df_encoded], axis=1).drop(columns=['operation_id'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(transfert_df[['is_verif']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['is_verif'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "transfert_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "transfert_df_encoded.index = transfert_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "transfert_df = pd.concat([transfert_df, transfert_df_encoded], axis=1).drop(columns=['is_verif'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(transfert_df[['is_secured']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['is_secured'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "transfert_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "transfert_df_encoded.index = transfert_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "transfert_df = pd.concat([transfert_df, transfert_df_encoded], axis=1).drop(columns=['is_secured'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "    #e) user\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['signup_mode']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['signup_mode'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['signup_mode'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['kyc_status']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['kyc_status'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['kyc_status'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['verif_phone']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['verif_phone'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['verif_phone'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['verif_user']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['verif_user'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['verif_user'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['active']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['active'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['active'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['verif_otp']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['verif_otp'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['verif_otp'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['is_account_delete']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['is_account_delete'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['is_account_delete'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['is_api_withdrawal']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['is_api_withdrawal'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['is_api_withdrawal'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['is_api_allow']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['is_api_allow'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['is_api_allow'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(user_df[['email_auth_enabled']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['email_auth_enabled'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "user_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "user_df_encoded.index = user_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "user_df = pd.concat([user_df, user_df_encoded], axis=1).drop(columns=['email_auth_enabled'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "    #proceed\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(proceed_df[['currency']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['currency'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "proceed_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "proceed_df_encoded.index = proceed_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "proceed_df = pd.concat([proceed_df, proceed_df_encoded], axis=1).drop(columns=['currency'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(proceed_df[['status']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['status'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "proceed_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "proceed_df_encoded.index = proceed_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "proceed_df = pd.concat([proceed_df, proceed_df_encoded], axis=1).drop(columns=['status'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(proceed_df[['operation_id']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['operation_id'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "proceed_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "proceed_df_encoded.index = proceed_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "proceed_df = pd.concat([proceed_df, proceed_df_encoded], axis=1).drop(columns=['operation_id'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(proceed_df[['is_verif']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['is_verif'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "proceed_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "proceed_df_encoded.index = proceed_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "proceed_df = pd.concat([proceed_df, proceed_df_encoded], axis=1).drop(columns=['is_verif'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "\n",
    "    #history\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(history_df[['status']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['status'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "history_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "history_df_encoded.index = history_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "history_df = pd.concat([history_df, history_df_encoded], axis=1).drop(columns=['status'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "    #bill\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(bill_df[['status']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['status'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "bill_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "bill_df_encoded.index = bill_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "bill_df = pd.concat([bill_df, bill_df_encoded], axis=1).drop(columns=['status'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(bill_df[['currency']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['currency'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "bill_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "bill_df_encoded.index = bill_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "bill_df = pd.concat([bill_df, bill_df_encoded], axis=1).drop(columns=['currency'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(bill_df[['operation_id']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['operation_id'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "bill_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "bill_df_encoded.index = bill_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "bill_df = pd.concat([bill_df, bill_df_encoded], axis=1).drop(columns=['operation_id'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')#creation de l'encodeur One-Hot\n",
    "encoded_features = encoder.fit_transform(bill_df[['is_verif']])#ajustement de l'encodeur et transformation des donnees\n",
    "features_names = encoder.get_feature_names_out(['is_verif'])#recuperation correcte des noms des nouvelles colonnes encodees\n",
    "bill_df_encoded = pd.DataFrame(encoded_features, columns=features_names)#creation d'un dataframe avec les nouvelles valeurs encodees\n",
    "bill_df_encoded.index = bill_df.index #reinitialisation de l'index pour correspondre au dataframe original\n",
    "bill_df = pd.concat([bill_df, bill_df_encoded], axis=1).drop(columns=['is_verif'])#concatenation avce le DataFrame original et supression de l'anciene colonne\n",
    "\n",
    "print(\"Encodage binaire termernier avec success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e7b5289-da93-4c78-82f0-a4bb6a506db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodage ordinal terminee avec success!\n"
     ]
    }
   ],
   "source": [
    "#label encoding(encodage ordinal)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#a)account.csv\n",
    "label_encoder = LabelEncoder()\n",
    "splogin_history_df['country'] = label_encoder.fit_transform(splogin_history_df['country'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "splogin_history_df['city'] = label_encoder.fit_transform(splogin_history_df['city'])\n",
    "\n",
    "#b) user\n",
    "label_encoder = LabelEncoder()\n",
    "user_df['last_host_location'] = label_encoder.fit_transform(user_df['last_host_location'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "user_df['idd_country'] = label_encoder.fit_transform(user_df['idd_country'])\n",
    "\n",
    "#c)history\n",
    "label_encoder = LabelEncoder()\n",
    "history_df['operation_id'] = label_encoder.fit_transform(history_df['operation_id'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "history_df['location'] = label_encoder.fit_transform(history_df['location'])\n",
    "\n",
    "print(\"Encodage ordinal terminee avec success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a453fc17-ac07-4531-93d1-2c86fc37e4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->Colonnes numeriques account.csv: Index(['id', 'user_id', 'wallet', 'balance', 'updated_at', 'trade_balance',\n",
      "       'currency_id_2.0', 'currency_id_3.0', 'currency_id_4.0',\n",
      "       'currency_id_5.0', 'currency_id_7.0', 'currency_id_8.0',\n",
      "       'currency_id_14.0', 'currency_id_15.0', 'currency_id_21.0'],\n",
      "      dtype='object')\n",
      "id: 8112 valeurs uniques\n",
      "user_id: 1668 valeurs uniques\n",
      "wallet: 6 valeurs uniques\n",
      "balance: 167 valeurs uniques\n",
      "updated_at: 6 valeurs uniques\n",
      "trade_balance: 65 valeurs uniques\n",
      "currency_id_2.0: 2 valeurs uniques\n",
      "currency_id_3.0: 2 valeurs uniques\n",
      "currency_id_4.0: 2 valeurs uniques\n",
      "currency_id_5.0: 2 valeurs uniques\n",
      "currency_id_7.0: 2 valeurs uniques\n",
      "currency_id_8.0: 2 valeurs uniques\n",
      "currency_id_14.0: 2 valeurs uniques\n",
      "currency_id_15.0: 2 valeurs uniques\n",
      "currency_id_21.0: 2 valeurs uniques\n",
      "-->Colonnes numeriques changes.csv: Index(['id', 'in_amount', 'user_id', 'in_wallet', 'out_wallet', 'out_amount',\n",
      "       'in_service_id', 'out_service_id', 'ext_id', 'proceed_at', 'other',\n",
      "       'external_reference', 'status_PENDING', 'status_RECEIVED',\n",
      "       'status_SUCCESS', 'is_verif_1', 'operation_id_7', 'operation_id_10'],\n",
      "      dtype='object')\n",
      "id: 2447 valeurs uniques\n",
      "in_amount: 755 valeurs uniques\n",
      "user_id: 127 valeurs uniques\n",
      "in_wallet: 29 valeurs uniques\n",
      "out_wallet: 32 valeurs uniques\n",
      "out_amount: 1342 valeurs uniques\n",
      "in_service_id: 28 valeurs uniques\n",
      "out_service_id: 28 valeurs uniques\n",
      "ext_id: 1 valeurs uniques\n",
      "proceed_at: 2 valeurs uniques\n",
      "other: 1 valeurs uniques\n",
      "external_reference: 2 valeurs uniques\n",
      "status_PENDING: 2 valeurs uniques\n",
      "status_RECEIVED: 2 valeurs uniques\n",
      "status_SUCCESS: 2 valeurs uniques\n",
      "is_verif_1: 2 valeurs uniques\n",
      "operation_id_7: 2 valeurs uniques\n",
      "operation_id_10: 2 valeurs uniques\n",
      "-->Colonnes numeriques splogin.csv: Index(['id', 'user_id', 'host', 'status_SUCCESS'], dtype='object')\n",
      "id: 1805 valeurs uniques\n",
      "user_id: 481 valeurs uniques\n",
      "host: 20 valeurs uniques\n",
      "status_SUCCESS: 2 valeurs uniques\n",
      "-->Colonnes numeriques transfert.csv: Index(['id', 'user_id', 'receiver_id', 'amount', 'proceed_at', 'ext_id',\n",
      "       'status_SUCCESS', 'currency_XAF', 'currency_XOF', 'is_verif_1'],\n",
      "      dtype='object')\n",
      "id: 168 valeurs uniques\n",
      "user_id: 30 valeurs uniques\n",
      "receiver_id: 38 valeurs uniques\n",
      "amount: 72 valeurs uniques\n",
      "proceed_at: 2 valeurs uniques\n",
      "ext_id: 1 valeurs uniques\n",
      "status_SUCCESS: 2 valeurs uniques\n",
      "currency_XAF: 2 valeurs uniques\n",
      "currency_XOF: 2 valeurs uniques\n",
      "is_verif_1: 2 valeurs uniques\n",
      "-->Colonnes numeriques user: Index(['id', 'username', 'date_start', 'last_operation', 'balance', 'nb_trans',\n",
      "       'contact', 'rewards_balance', 'matricule', 'last_login', 'host_city',\n",
      "       'signup_mode_PERSONNAL', 'signup_mode_PROFESSIONAL',\n",
      "       'signup_mode_UNKNOWN', 'kyc_status_NOT_INITIATED', 'kyc_status_PENDING',\n",
      "       'kyc_status_REJECTED', 'verif_phone_1', 'verif_user_1', 'active_1',\n",
      "       'verif_otp_1', 'is_account_delete_1', 'is_api_withdrawal_1',\n",
      "       'is_api_allow_1', 'email_auth_enabled_1'],\n",
      "      dtype='object')\n",
      "id: 2677 valeurs uniques\n",
      "username: 4 valeurs uniques\n",
      "date_start: 2 valeurs uniques\n",
      "last_operation: 3 valeurs uniques\n",
      "balance: 148 valeurs uniques\n",
      "nb_trans: 74 valeurs uniques\n",
      "contact: 3 valeurs uniques\n",
      "rewards_balance: 103 valeurs uniques\n",
      "matricule: 2 valeurs uniques\n",
      "last_login: 3 valeurs uniques\n",
      "host_city: 34 valeurs uniques\n",
      "signup_mode_PERSONNAL: 2 valeurs uniques\n",
      "signup_mode_PROFESSIONAL: 2 valeurs uniques\n",
      "signup_mode_UNKNOWN: 2 valeurs uniques\n",
      "kyc_status_NOT_INITIATED: 2 valeurs uniques\n",
      "kyc_status_PENDING: 2 valeurs uniques\n",
      "kyc_status_REJECTED: 2 valeurs uniques\n",
      "verif_phone_1: 2 valeurs uniques\n",
      "verif_user_1: 2 valeurs uniques\n",
      "active_1: 2 valeurs uniques\n",
      "verif_otp_1: 2 valeurs uniques\n",
      "is_account_delete_1: 2 valeurs uniques\n",
      "is_api_withdrawal_1: 2 valeurs uniques\n",
      "is_api_allow_1: 2 valeurs uniques\n",
      "email_auth_enabled_1: 2 valeurs uniques\n",
      "-->Colonnes numeriques proceed: Index(['id', 'service_id', 'user_id', 'ext_id', 'amount', 'wallet',\n",
      "       'proceed_at', 'other', 'external_reference', 'currency_CDF',\n",
      "       'currency_DOGE', 'currency_LTC', 'currency_USD', 'currency_XAF',\n",
      "       'currency_XOF', 'status_PENDING', 'status_PROCESSING',\n",
      "       'status_RECEIVED', 'status_SUCCESS', 'operation_id_4', 'is_verif_1',\n",
      "       'is_verif_2'],\n",
      "      dtype='object')\n",
      "id: 10063 valeurs uniques\n",
      "service_id: 32 valeurs uniques\n",
      "user_id: 307 valeurs uniques\n",
      "ext_id: 2 valeurs uniques\n",
      "amount: 1081 valeurs uniques\n",
      "wallet: 60 valeurs uniques\n",
      "proceed_at: 4 valeurs uniques\n",
      "other: 8 valeurs uniques\n",
      "external_reference: 5 valeurs uniques\n",
      "currency_CDF: 2 valeurs uniques\n",
      "currency_DOGE: 2 valeurs uniques\n",
      "currency_LTC: 2 valeurs uniques\n",
      "currency_USD: 2 valeurs uniques\n",
      "currency_XAF: 2 valeurs uniques\n",
      "currency_XOF: 2 valeurs uniques\n",
      "status_PENDING: 2 valeurs uniques\n",
      "status_PROCESSING: 2 valeurs uniques\n",
      "status_RECEIVED: 2 valeurs uniques\n",
      "status_SUCCESS: 2 valeurs uniques\n",
      "operation_id_4: 2 valeurs uniques\n",
      "is_verif_1: 2 valeurs uniques\n",
      "is_verif_2: 2 valeurs uniques\n",
      "--->Colonnes numeriques history.csv: Index(['id', 'document', 'date', 'value', 'user_id', 'account_id', 'ext_id',\n",
      "       'host', 'city', 'prev_account_balance', 'current_account_balance',\n",
      "       'status_FAILLURE', 'status_FAILURE', 'status_PENDING',\n",
      "       'status_PROCESSING', 'status_RECEIVED', 'status_REFUNDED',\n",
      "       'status_SUCCE3SS', 'status_SUCCESS'],\n",
      "      dtype='object')\n",
      "id: 35028 valeurs uniques\n",
      "document: 16 valeurs uniques\n",
      "date: 9 valeurs uniques\n",
      "value: 3585 valeurs uniques\n",
      "user_id: 414 valeurs uniques\n",
      "account_id: 605 valeurs uniques\n",
      "ext_id: 27 valeurs uniques\n",
      "host: 90 valeurs uniques\n",
      "city: 73 valeurs uniques\n",
      "prev_account_balance: 10862 valeurs uniques\n",
      "current_account_balance: 11375 valeurs uniques\n",
      "status_FAILLURE: 2 valeurs uniques\n",
      "status_FAILURE: 2 valeurs uniques\n",
      "status_PENDING: 2 valeurs uniques\n",
      "status_PROCESSING: 2 valeurs uniques\n",
      "status_RECEIVED: 2 valeurs uniques\n",
      "status_REFUNDED: 2 valeurs uniques\n",
      "status_SUCCE3SS: 2 valeurs uniques\n",
      "status_SUCCESS: 2 valeurs uniques\n",
      "-->Colonnes numeriques bill.csv: Index(['id', 'amount', 'service_id', 'user_id', 'ext_id', 'proceed_at',\n",
      "       'wallet', 'other', 'qr_id', 'payer', 'order_id', 'customer_id',\n",
      "       'external_reference', 'payer_email', 'status_FAILURE', 'status_PENDING',\n",
      "       'status_PROCESSING', 'status_RECEIVED', 'status_SUCCESS',\n",
      "       'currency_CDF', 'currency_DOGE', 'currency_LTC', 'currency_SPC',\n",
      "       'currency_USD', 'currency_XAF', 'currency_XOF', 'currency_ZMW',\n",
      "       'is_verif_1'],\n",
      "      dtype='object')\n",
      "id: 20706 valeurs uniques\n",
      "amount: 1421 valeurs uniques\n",
      "service_id: 35 valeurs uniques\n",
      "user_id: 127 valeurs uniques\n",
      "ext_id: 1 valeurs uniques\n",
      "proceed_at: 7 valeurs uniques\n",
      "wallet: 69 valeurs uniques\n",
      "other: 4 valeurs uniques\n",
      "qr_id: 260 valeurs uniques\n",
      "payer: 52 valeurs uniques\n",
      "order_id: 22 valeurs uniques\n",
      "customer_id: 5890 valeurs uniques\n",
      "external_reference: 2 valeurs uniques\n",
      "payer_email: 57 valeurs uniques\n",
      "status_FAILURE: 2 valeurs uniques\n",
      "status_PENDING: 2 valeurs uniques\n",
      "status_PROCESSING: 2 valeurs uniques\n",
      "status_RECEIVED: 2 valeurs uniques\n",
      "status_SUCCESS: 2 valeurs uniques\n",
      "currency_CDF: 2 valeurs uniques\n",
      "currency_DOGE: 2 valeurs uniques\n",
      "currency_LTC: 2 valeurs uniques\n",
      "currency_SPC: 2 valeurs uniques\n",
      "currency_USD: 2 valeurs uniques\n",
      "currency_XAF: 2 valeurs uniques\n",
      "currency_XOF: 2 valeurs uniques\n",
      "currency_ZMW: 2 valeurs uniques\n",
      "is_verif_1: 2 valeurs uniques\n"
     ]
    }
   ],
   "source": [
    "#Normalisation des variables numeriques\n",
    "#1) identifications des colonnes numeriques\n",
    "numeric_columns = account_df.select_dtypes(include=['int64', 'float64']).columns #account.csv\n",
    "print(\"-->Colonnes numeriques account.csv:\", numeric_columns)\n",
    "for col in numeric_columns:\n",
    "    print(f\"{col}: {account_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "numeric_columns = changes_df.select_dtypes(include=['int64', 'float64']).columns #chasnges.csv\n",
    "print(\"-->Colonnes numeriques changes.csv:\", numeric_columns)\n",
    "for col in numeric_columns:\n",
    "    print(f\"{col}: {changes_df[col].nunique()} valeurs uniques\")\n",
    "    \n",
    "numeric_columns = splogin_history_df.select_dtypes(include=['int64', 'float64']).columns #splogin_history.csv\n",
    "print(\"-->Colonnes numeriques splogin.csv:\", numeric_columns)\n",
    "for col in numeric_columns:\n",
    "    print(f\"{col}: {splogin_history_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "numeric_columns = transfert_df.select_dtypes(include=['int64', 'float64']).columns #tranfert.csv\n",
    "print(\"-->Colonnes numeriques transfert.csv:\", numeric_columns)\n",
    "for col in numeric_columns:\n",
    "    print(f\"{col}: {transfert_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "numeric_columns = user_df.select_dtypes(include=['int64', 'float64']).columns #user.csv\n",
    "print(\"-->Colonnes numeriques user:\", numeric_columns)\n",
    "for col in numeric_columns:\n",
    "    print(f\"{col}: {user_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "numeric_columns = proceed_df.select_dtypes(include=['int64', 'float64']).columns #proceed.csv\n",
    "print(\"-->Colonnes numeriques proceed:\", numeric_columns)\n",
    "for col in numeric_columns:\n",
    "    print(f\"{col}: {proceed_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "numeric_columns = history_df.select_dtypes(include=['int64', 'float64']).columns #history.csv\n",
    "print(\"--->Colonnes numeriques history.csv:\", numeric_columns)\n",
    "for col in numeric_columns:\n",
    "    print(f\"{col}: {history_df[col].nunique()} valeurs uniques\")\n",
    "\n",
    "numeric_columns = bill_df.select_dtypes(include=['int64', 'float64']).columns #bill.csv\n",
    "print(\"-->Colonnes numeriques bill.csv:\", numeric_columns)\n",
    "for col in numeric_columns:\n",
    "    print(f\"{col}: {bill_df[col].nunique()} valeurs uniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1b4b7c8-724e-469e-87bd-b95a8d939f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalisation effectuees avec success!\n"
     ]
    }
   ],
   "source": [
    "#normalisation des colonnes numeriques\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#1) account.csv\n",
    "account_df[['id']] = scaler.fit_transform(account_df[['id']])\n",
    "account_df[['user_id']] = scaler.fit_transform(account_df[['user_id']])\n",
    "account_df[['balance']] = scaler.fit_transform(account_df[['balance']])\n",
    "account_df[['trade_balance']] = scaler.fit_transform(account_df[['trade_balance']])\n",
    "\n",
    "#2) changes.csv\n",
    "changes_df[['id']] = scaler.fit_transform(changes_df[['id']])\n",
    "changes_df[['user_id']] = scaler.fit_transform(changes_df[['user_id']])\n",
    "changes_df[['in_amount']] = scaler.fit_transform(changes_df[['in_amount']])\n",
    "changes_df[['out_amount']] = scaler.fit_transform(changes_df[['out_amount']])\n",
    "changes_df[['in_service_id']] = scaler.fit_transform(changes_df[['in_service_id']])\n",
    "changes_df[['out_service_id']] = scaler.fit_transform(changes_df[['out_service_id']])\n",
    "\n",
    "#3)splogin_history\n",
    "splogin_history_df[['id']] = scaler.fit_transform(splogin_history_df[['id']])\n",
    "splogin_history_df[['user_id']] = scaler.fit_transform(splogin_history_df[['user_id']])\n",
    "\n",
    "#4)transfert.csv\n",
    "transfert_df[['id']] = scaler.fit_transform(transfert_df[['id']])\n",
    "transfert_df[['user_id']] = scaler.fit_transform(transfert_df[['user_id']])\n",
    "transfert_df[['receiver_id']] = scaler.fit_transform(transfert_df[['receiver_id']])\n",
    "transfert_df[['amount']] = scaler.fit_transform(transfert_df[['amount']])\n",
    "\n",
    "#5)user.csv\n",
    "user_df[['id']] = scaler.fit_transform(user_df[['id']])\n",
    "user_df[['balance']] = scaler.fit_transform(user_df[['balance']])\n",
    "user_df[['nb_trans']] = scaler.fit_transform(user_df[['nb_trans']])\n",
    "user_df[['rewards_balance']] = scaler.fit_transform(user_df[['rewards_balance']])\n",
    "\n",
    "#6)proceed.csv\n",
    "proceed_df[['id']] = scaler.fit_transform(proceed_df[['id']])\n",
    "proceed_df[['service_id']] = scaler.fit_transform(proceed_df[['service_id']])\n",
    "proceed_df[['user_id']] = scaler.fit_transform(proceed_df[['user_id']])\n",
    "proceed_df[['amount']] = scaler.fit_transform(proceed_df[['amount']])\n",
    "\n",
    "#7)history.csv\n",
    "history_df[['id']] = scaler.fit_transform(history_df[['id']])\n",
    "history_df[['value']] = scaler.fit_transform(history_df[['value']])\n",
    "history_df[['user_id']] = scaler.fit_transform(history_df[['user_id']])\n",
    "history_df[['account_id']] = scaler.fit_transform(history_df[['account_id']])\n",
    "history_df[['prev_account_balance']] = scaler.fit_transform(history_df[['prev_account_balance']])\n",
    "history_df[['current_account_balance']] = scaler.fit_transform(history_df[['current_account_balance']])\n",
    "\n",
    "#8) bill.csv\n",
    "bill_df[['id']] = scaler.fit_transform(bill_df[['id']])\n",
    "bill_df[['service_id']] = scaler.fit_transform(bill_df[['service_id']])\n",
    "bill_df[['user_id']] = scaler.fit_transform(bill_df[['user_id']])\n",
    "bill_df[['amount']] = scaler.fit_transform(bill_df[['amount']])\n",
    "bill_df[['qr_id']] = scaler.fit_transform(bill_df[['qr_id']])\n",
    "bill_df[['customer_id']] = scaler.fit_transform(bill_df[['customer_id']])\n",
    "\n",
    "print(\"Normalisation effectuees avec success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e9ca548-9790-47fb-9f0e-bd6d72268de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier sauvegarde: account_cleaned.csv\n",
      "Fichier sauvegarde: changes_cleaned.csv\n",
      "Fichier sauvegarde: splogin_cleaned.csv\n",
      "Fichier sauvegarde: transfert_cleaned.csv\n",
      "Fichier sauvegarde: user_cleaned.csv\n",
      "Fichier sauvegarde: proceed_cleaned.csv\n",
      "Fichier sauvegarde: history_cleaned.csv\n",
      "Fichier sauvegarde: bill_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "#Sauvegarde des fichiers nettoyes\n",
    "dataframes = {\n",
    "    \"account_cleaned.csv\": account_df,\n",
    "    \"changes_cleaned.csv\": changes_df,\n",
    "    \"splogin_cleaned.csv\": splogin_history_df,\n",
    "    \"transfert_cleaned.csv\": transfert_df,\n",
    "    \"user_cleaned.csv\": user_df,\n",
    "    \"proceed_cleaned.csv\": proceed_df,\n",
    "    \"history_cleaned.csv\": history_df,\n",
    "    \"bill_cleaned.csv\": bill_df\n",
    "}\n",
    "for filename, df in dataframes.items():\n",
    "    account_df.to_csv(filename, index=False)\n",
    "    print(f\"Fichier sauvegarde: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fdf62-8df9-4004-8266-6d6a692f3970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
